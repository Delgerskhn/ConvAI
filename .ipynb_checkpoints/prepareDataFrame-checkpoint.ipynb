{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining data frame variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from removeStopWords.ipynb\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 112522819 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3078c00fce74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremoveStopWords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremoveStopWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/train_self_original.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mrowList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Pandas-Demo/ConvAI/removeStopWords.ipynb\u001b[0m in \u001b[0;36mremoveStopWords\u001b[0;34m(text)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \"\"\"\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    439\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 112522819 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import import_ipynb\n",
    "import removeStopWords\n",
    "import io\n",
    "\n",
    "data = removeStopWords.removeStopWords(open(\"../data/train_self_original.txt\", \"r\").read())\n",
    "buf = io.StringIO(msg)\n",
    "rowList = buf.readlines()\n",
    "\n",
    "conversations = {\n",
    "    \"conv_id\": []\n",
    "}\n",
    "chats = {\n",
    "    \"conv_id\": [],\n",
    "    \"buddy1\": [],\n",
    "    \"chat_id\": [],\n",
    "    \"buddy2\": []\n",
    "}\n",
    "personals = {\n",
    "    \"persona\": [],\n",
    "    \"personaId\": [],\n",
    "    \"conv_id\": []\n",
    "}\n",
    "answers = {\n",
    "    \"conv_id\": [],\n",
    "    \"chat_id\": [],\n",
    "    \"ans_id\": [],\n",
    "    \"answer\": []\n",
    "}\n",
    "isPersona = lambda rowContext : rowContext.find(\"your persona:\")!=-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseAnswer(conv_id, chat_id, answersContext):\n",
    "    returnind = answersContext.index(\"\\n\")\n",
    "    answersContext=answersContext[0:returnind]\n",
    "    answersList = answersContext.split(\"|\")\n",
    "    for i in range(0, len(answersList)):\n",
    "        answers[\"conv_id\"].append(conv_id)\n",
    "        answers[\"chat_id\"].append(chat_id)\n",
    "        answers[\"ans_id\"].append(len(answers[\"ans_id\"])+1)\n",
    "        answers[\"answer\"].append(answersList[i])        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseChat(currentContext, conv_id):\n",
    "    buddy1EndPointPattern = \"\\t\"\n",
    "    buddy1EndPoint = currentContext.index(buddy1EndPointPattern)\n",
    "    buddy2EndPointPattern =\"\\t\\t\"\n",
    "    buddy2EndPoint = currentContext.index(buddy2EndPointPattern)\n",
    "    buddy1str = currentContext[2:buddy1EndPoint]\n",
    "    chats[\"conv_id\"].append(conv_id)\n",
    "    chats[\"buddy1\"].append(buddy1str)\n",
    "    chats[\"chat_id\"].append(len(chats[\"chat_id\"])+1)\n",
    "    chats[\"buddy2\"].append(currentContext[buddy1EndPoint+len(buddy1EndPointPattern):buddy2EndPoint])\n",
    "    answersContext = currentContext[buddy2EndPoint+len(buddy2EndPointPattern):]\n",
    "    parseAnswer(conv_id, chats[\"chat_id\"][-1], answersContext)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsePersona(conv_id, persona):\n",
    "    personals[\"persona\"].append(slicePersona(persona))\n",
    "    personals[\"personaId\"].append(len(personals[\"personaId\"])+1)\n",
    "    personals[\"conv_id\"].append(conv_id)\n",
    "\n",
    "def slicePersona(persona) :\n",
    "    pattern = \"your persona:\"\n",
    "    prefixInd = persona.rindex(pattern)\n",
    "    retInd = persona.index(\"\\n\")\n",
    "    return persona[prefixInd+len(pattern):retInd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseConversation(i):\n",
    "    personaLen=0\n",
    "    personaIt = i\n",
    "    while(isPersona(rowList[personaIt]) and personaIt < len(rowList)):\n",
    "        personaLen+=1\n",
    "        personaIt+=1\n",
    "    conversations[\"conv_id\"].append(len(conversations[\"conv_id\"])+1)\n",
    "    \n",
    "    for persona in rowList[i:i+personaLen] :\n",
    "        parsePersona(conversations[\"conv_id\"][-1], persona)\n",
    "    return personaLen-1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start converting information from txt file and creating dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseObjects():\n",
    "    skipPersona = lambda index, personaLen : index+int(personaLen)\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(rowList):\n",
    "        if isPersona(rowList[i]) :\n",
    "            personaLen = parseConversation(i)\n",
    "            i = skipPersona(i, personaLen)\n",
    "        else:\n",
    "            parseChat(rowList[i], conversations[\"conv_id\"][-1])\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseObjects()\n",
    "convdf = pd.DataFrame(conversations, columns=[\"conv_id\"])\n",
    "personadf = pd.DataFrame(personals, columns=[\"conv_id\", \"persona\",\"personaId\"])\n",
    "chatdf = pd.DataFrame(chats, columns=[\"conv_id\", \"buddy1\", \"chat_id\", \"buddy2\"])\n",
    "answerdf = pd.DataFrame(answers, columns = [\"conv_id\", \"chat_id\", \"ans_id\", \"answer\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the most common personal informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
